{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f01040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium) (2.2.4)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-3.1.1 farama-notifications-0.0.4 gymnasium-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a31a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stable-baselines3\n",
      "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from stable-baselines3) (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3) (2.2.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3) (2.6.0+cpu)\n",
      "Requirement already satisfied: cloudpickle in /home/codespace/.python/current/lib/python3.12/site-packages (from stable-baselines3) (3.1.1)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3) (3.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->stable-baselines3) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->stable-baselines3) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n",
      "Downloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
      "Installing collected packages: stable-baselines3\n",
      "Successfully installed stable-baselines3-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stable-baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42528b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[toy-text] in /home/codespace/.python/current/lib/python3.12/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium[toy-text]) (2.2.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[toy-text]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium[toy-text]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium[toy-text]) (0.0.4)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[toy-text])\n",
      "  Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Downloading pygame-2.6.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium[toy-text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e7d4af-9548-43aa-b55a-75bb5428be5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /home/codespace/.python/current/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from stable-baselines3[extra]) (1.1.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.20 in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.2.4)\n",
      "Requirement already satisfied: torch<3.0,>=2.3 in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.6.0+cpu)\n",
      "Requirement already satisfied: cloudpickle in /home/codespace/.python/current/lib/python3.12/site-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3[extra]) (3.10.1)\n",
      "Collecting opencv-python (from stable-baselines3[extra])\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pygame in /home/codespace/.python/current/lib/python3.12/site-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra])\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3[extra]) (7.0.0)\n",
      "Collecting tqdm (from stable-baselines3[extra])\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting rich (from stable-baselines3[extra])\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
      "  Downloading ale_py-0.10.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: pillow in /home/codespace/.local/lib/python3.12/site-packages (from stable-baselines3[extra]) (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (76.0.0)\n",
      "Requirement already satisfied: six>1.9 in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.13.1)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->stable-baselines3[extra]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas->stable-baselines3[extra]) (2025.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->stable-baselines3[extra])\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra])\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
      "Downloading ale_py-0.10.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading grpcio-1.71.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: werkzeug, tqdm, tensorboard-data-server, protobuf, opencv-python, mdurl, markdown, grpcio, ale-py, absl-py, tensorboard, markdown-it-py, rich\n",
      "Successfully installed absl-py-2.2.2 ale-py-0.10.2 grpcio-1.71.0 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 opencv-python-4.11.0.86 protobuf-6.30.2 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tqdm-4.67.1 werkzeug-3.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.envs import SimpleMultiObsEnv\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from pathlib import Path\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch as t\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple\n",
    "from gym.spaces import Discrete, Box\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gymnasium.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# %load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedb949",
   "metadata": {},
   "source": [
    "*Parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aaadd2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(NamedTuple):\n",
    "    total_episodes: int  # Total episodes\n",
    "    learning_rate: float  # Learning rate\n",
    "    gamma: float  # Discounting rate\n",
    "    epsilon: float  # Exploration probability\n",
    "    map_size: int  # Number of tiles of one side of the squared environment\n",
    "    seed: int  # Define a seed so that we get reproducible results\n",
    "    is_slippery: bool  # If true the player will move in intended direction with probability of 1/3 else will move in either perpendicular direction with equal probability of 1/3 in both directions\n",
    "    n_runs: int  # Number of runs\n",
    "    action_size: int  # Number of possible actions\n",
    "    state_size: int  # Number of possible states\n",
    "    proba_frozen: float  # Probability that a tile is frozen\n",
    "    savefig_folder: Path  # Root folder where plots are saved\n",
    "\n",
    "\n",
    "params = Params(\n",
    "    total_episodes=2000,\n",
    "    learning_rate=0.8,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.1,\n",
    "    map_size=5,\n",
    "    seed=123,\n",
    "    is_slippery=False,\n",
    "    n_runs=20,\n",
    "    action_size=None,\n",
    "    state_size=None,\n",
    "    proba_frozen=0.9,\n",
    "    savefig_folder=Path(\"../tutorials/\"),\n",
    ")\n",
    "params\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(params.seed)\n",
    "\n",
    "# Create the figure folder if it doesn't exist\n",
    "params.savefig_folder.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b25cb5",
   "metadata": {},
   "source": [
    "*Env*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a492ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    is_slippery=params.is_slippery,\n",
    "    render_mode=\"rgb_array\",\n",
    "    desc=generate_random_map(\n",
    "        size=params.map_size, p=params.proba_frozen, seed=params.seed\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15805944",
   "metadata": {},
   "source": [
    "*Q-table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376d503f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size: 4\n",
      "State size: 25\n"
     ]
    }
   ],
   "source": [
    "params = params._replace(action_size=env.action_space.n)\n",
    "params = params._replace(state_size=env.observation_space.n)\n",
    "print(f\"Action size: {params.action_size}\")\n",
    "print(f\"State size: {params.state_size}\")\n",
    "\n",
    "\n",
    "class ppo:\n",
    "    def __init__(self, learning_rate, gamma, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.reset_qtable()\n",
    "\n",
    "    def update(self, state, action, reward, new_state):\n",
    "        \"\"\"Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\"\"\"\n",
    "        delta = (\n",
    "            reward\n",
    "            + self.gamma * np.max(self.qtable[new_state, :])\n",
    "            - self.qtable[state, action]\n",
    "        )\n",
    "        q_update = self.qtable[state, action] + self.learning_rate * delta\n",
    "        return q_update\n",
    "\n",
    "    def reset_qtable(self):\n",
    "        \"\"\"Reset the Q-table.\"\"\"\n",
    "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def choose_action(self, action_space, state, qtable):\n",
    "        \"\"\"Choose an action `a` in the current world state (s).\"\"\"\n",
    "        # First we randomize a number\n",
    "        explor_exploit_tradeoff = rng.uniform(0, 1)\n",
    "\n",
    "        # Exploration\n",
    "        if explor_exploit_tradeoff < self.epsilon:\n",
    "            action = action_space.sample()\n",
    "\n",
    "        # Exploitation (taking the biggest Q-value for this state)\n",
    "        else:\n",
    "            # Break ties randomly\n",
    "            # Find the indices where the Q-value equals the maximum value\n",
    "            # Choose a random action from the indices where the Q-value is maximum\n",
    "            max_ids = np.where(qtable[state, :] == max(qtable[state, :]))[0]\n",
    "            action = rng.choice(max_ids)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa0f8f1",
   "metadata": {},
   "source": [
    "*Instantiate learner-explorer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "246178b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = ppo(\n",
    "    learning_rate=params.learning_rate,\n",
    "    gamma=params.gamma,\n",
    "    state_size=params.state_size,\n",
    "    action_size=params.action_size,\n",
    ")\n",
    "explorer = EpsilonGreedy(\n",
    "    epsilon=params.epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537bfa3f",
   "metadata": {},
   "source": [
    "*main function to run our environment until the maximum number of episodes params.total_episodes. To account for stochasticity,  also run our environment a few times.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75cd720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_env():\n",
    "    rewards = np.zeros((params.total_episodes, params.n_runs))\n",
    "    steps = np.zeros((params.total_episodes, params.n_runs))\n",
    "    episodes = np.arange(params.total_episodes)\n",
    "    qtables = np.zeros((params.n_runs, params.state_size, params.action_size))\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "\n",
    "    for run in range(params.n_runs):  # Run several times to account for stochasticity\n",
    "        learner.reset_qtable()  # Reset the Q-table between runs\n",
    "\n",
    "        for episode in ppo(\n",
    "            episodes, desc=f\"Run {run}/{params.n_runs} - Episodes\", leave=False\n",
    "        ):\n",
    "            state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "            step = 0\n",
    "            done = False\n",
    "            total_rewards = 0\n",
    "\n",
    "            while not done:\n",
    "                action = explorer.choose_action(\n",
    "                    action_space=env.action_space, state=state, qtable=learner.qtable\n",
    "                )\n",
    "\n",
    "                # Log all states and actions\n",
    "                all_states.append(state)\n",
    "                all_actions.append(action)\n",
    "\n",
    "                # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "                new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                learner.qtable[state, action] = learner.update(\n",
    "                    state, action, reward, new_state\n",
    "                )\n",
    "\n",
    "                total_rewards += reward\n",
    "                step += 1\n",
    "\n",
    "                # Our new state is state\n",
    "                state = new_state\n",
    "\n",
    "            # Log all rewards and steps\n",
    "            rewards[episode, run] = total_rewards\n",
    "            steps[episode, run] = step\n",
    "        qtables[run, :, :] = learner.qtable\n",
    "\n",
    "    return rewards, steps, episodes, qtables, all_states, all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de5216",
   "metadata": {},
   "source": [
    "*Vis*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1c0551",
   "metadata": {},
   "source": [
    "*to plot the results with Seaborn, save the main results of the simulation in Pandas dataframes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc5593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(episodes, params, rewards, steps, map_size):\n",
    "    \"\"\"Convert the results of the simulation in dataframes.\"\"\"\n",
    "    res = pd.DataFrame(\n",
    "        data={\n",
    "            \"Episodes\": np.tile(episodes, reps=params.n_runs),\n",
    "            \"Rewards\": rewards.flatten(order=\"F\"),\n",
    "            \"Steps\": steps.flatten(order=\"F\"),\n",
    "        }\n",
    "    )\n",
    "    res[\"cum_rewards\"] = rewards.cumsum(axis=0).flatten(order=\"F\")\n",
    "    res[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", res.shape[0])\n",
    "\n",
    "    st = pd.DataFrame(data={\"Episodes\": episodes, \"Steps\": steps.mean(axis=1)})\n",
    "    st[\"map_size\"] = np.repeat(f\"{map_size}x{map_size}\", st.shape[0])\n",
    "    return res, st"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c8b8f",
   "metadata": {},
   "source": [
    "*plot the policy the agent has learned in the end. To do that we will: 1. extract the best Q-values from the Q-table for each state, 2. get the corresponding best action for those Q-values, 3. map each action to an arrow so we can visualize it*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce4fc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qtable_directions_map(qtable, map_size):\n",
    "    \"\"\"Get the best learned action & map it to arrows.\"\"\"\n",
    "    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\n",
    "    qtable_best_action = np.argmax(qtable, axis=1).reshape(map_size, map_size)\n",
    "    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "    qtable_directions = np.empty(qtable_best_action.flatten().shape, dtype=str)\n",
    "    eps = np.finfo(float).eps  # Minimum float number on the machine\n",
    "    for idx, val in enumerate(qtable_best_action.flatten()):\n",
    "        if qtable_val_max.flatten()[idx] > eps:\n",
    "            # Assign an arrow only if a minimal Q-value has been learned as best action\n",
    "            # otherwise since 0 is a direction, it also gets mapped on the tiles where\n",
    "            # it didn't actually learn anything\n",
    "            qtable_directions[idx] = directions[val]\n",
    "    qtable_directions = qtable_directions.reshape(map_size, map_size)\n",
    "    return qtable_val_max, qtable_directions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacccd0a",
   "metadata": {},
   "source": [
    "*plot on the left the last frame of the simulation. If the agent learned a good policy to solve the task, we expect to see it on the tile of the treasure in the last frame of the video. On the right we’ll plot the policy the agent has learned. Each arrow will represent the best action to choose for each tile/state*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efd45012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_q_values_map(qtable, env, map_size):\n",
    "    \"\"\"Plot the last frame of the simulation and the policy learned.\"\"\"\n",
    "    qtable_val_max, qtable_directions = qtable_directions_map(qtable, map_size)\n",
    "\n",
    "    # Plot the last frame\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    ax[0].imshow(env.render())\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"Last frame\")\n",
    "\n",
    "    # Plot the policy\n",
    "    sns.heatmap(\n",
    "        qtable_val_max,\n",
    "        annot=qtable_directions,\n",
    "        fmt=\"\",\n",
    "        ax=ax[1],\n",
    "        cmap=sns.color_palette(\"Blues\", as_cmap=True),\n",
    "        linewidths=0.7,\n",
    "        linecolor=\"black\",\n",
    "        xticklabels=[],\n",
    "        yticklabels=[],\n",
    "        annot_kws={\"fontsize\": \"xx-large\"},\n",
    "    ).set(title=\"Learned Q-values\\nArrows represent best action\")\n",
    "    for _, spine in ax[1].spines.items():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.7)\n",
    "        spine.set_color(\"black\")\n",
    "    img_title = f\"frozenlake_q_values_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c877a",
   "metadata": {},
   "source": [
    "*plot the distributions of states and actions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e56e522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_states_actions_distribution(states, actions, map_size):\n",
    "    \"\"\"Plot the distributions of states and actions.\"\"\"\n",
    "    labels = {\"LEFT\": 0, \"DOWN\": 1, \"RIGHT\": 2, \"UP\": 3}\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.histplot(data=states, ax=ax[0], kde=True)\n",
    "    ax[0].set_title(\"States\")\n",
    "    sns.histplot(data=actions, ax=ax[1])\n",
    "    ax[1].set_xticks(list(labels.values()), labels=labels.keys())\n",
    "    ax[1].set_title(\"Actions\")\n",
    "    fig.tight_layout()\n",
    "    img_title = f\"frozenlake_states_actions_distrib_{map_size}x{map_size}.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef767c3",
   "metadata": {},
   "source": [
    "*Run agent on += sizes*\n",
    "\n",
    "*DOWN and RIGHT actions get chosen more often, which makes sense as the agent starts at the top left of the map and needs to find its way down to the bottom right. Also the bigger the map, the less states/tiles further away from the starting state get visited.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b49a32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `MovingTargetEnv` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameNotFound\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m st_all = pd.DataFrame()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m map_size \u001b[38;5;129;01min\u001b[39;00m map_sizes:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     env = \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mMovingTargetEnv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_slippery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_slippery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrgb_array\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerate_random_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m            \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmap_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproba_frozen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     params = params._replace(action_size=env.action_space.n)\n\u001b[32m     16\u001b[39m     params = params._replace(state_size=env.observation_space.n)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gymnasium/envs/registration.py:689\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    688\u001b[39m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m689\u001b[39m     env_spec = \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gymnasium/envs/registration.py:533\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(env_id)\u001b[39m\n\u001b[32m    527\u001b[39m     logger.warn(\n\u001b[32m    528\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    529\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    530\u001b[39m     )\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m    535\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    536\u001b[39m     )\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gymnasium/envs/registration.py:399\u001b[39m, in \u001b[36m_check_version_exists\u001b[39m\u001b[34m(ns, name, version)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gymnasium/envs/registration.py:376\u001b[39m, in \u001b[36m_check_name_exists\u001b[39m\u001b[34m(ns, name)\u001b[39m\n\u001b[32m    373\u001b[39m namespace_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m suggestion_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error.NameNotFound(\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[31mNameNotFound\u001b[39m: Environment `MovingTargetEnv` doesn't exist."
     ]
    }
   ],
   "source": [
    "map_sizes = [3, 5, 7]\n",
    "res_all = pd.DataFrame()\n",
    "st_all = pd.DataFrame()\n",
    "\n",
    "for map_size in map_sizes:\n",
    "    env = gym.make(\n",
    "        \"MovingTargetEnv\",\n",
    "        is_slippery=params.is_slippery,\n",
    "        render_mode=\"rgb_array\",\n",
    "        desc=generate_random_map(\n",
    "            size=map_size, p=params.proba_frozen, seed=params.seed\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    params = params._replace(action_size=env.action_space.n)\n",
    "    params = params._replace(state_size=env.observation_space.n)\n",
    "    env.action_space.seed(\n",
    "        params.seed\n",
    "    )  # Set the seed to get reproducible results when sampling the action space\n",
    "    learner = ppo(\n",
    "        learning_rate=params.learning_rate,\n",
    "        gamma=params.gamma,\n",
    "        state_size=params.state_size,\n",
    "        action_size=params.action_size,\n",
    "    )\n",
    "    explorer = EpsilonGreedy(\n",
    "        epsilon=params.epsilon,\n",
    "    )\n",
    "\n",
    "    print(f\"Map size: {map_size}x{map_size}\")\n",
    "    rewards, steps, episodes, qtables, all_states, all_actions = run_env()\n",
    "\n",
    "    # Save the results in dataframes\n",
    "    res, st = postprocess(episodes, params, rewards, steps, map_size)\n",
    "    res_all = pd.concat([res_all, res])\n",
    "    st_all = pd.concat([st_all, st])\n",
    "    qtable = qtables.mean(axis=0)  # Average the Q-table between runs\n",
    "\n",
    "    plot_states_actions_distribution(\n",
    "        states=all_states, actions=all_actions, map_size=map_size\n",
    "    )  # Sanity check\n",
    "    plot_q_values_map(qtable, env, map_size)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90eab373",
   "metadata": {},
   "source": [
    "*check if our agent is learning, we want to plot the cumulated sum of rewards, as well as the number of steps needed until the end of the episode*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps_and_rewards(rewards_df, steps_df):\n",
    "    \"\"\"Plot the steps and rewards from dataframes.\"\"\"\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "    sns.lineplot(\n",
    "        data=rewards_df, x=\"Episodes\", y=\"cum_rewards\", hue=\"map_size\", ax=ax[0]\n",
    "    )\n",
    "    ax[0].set(ylabel=\"Cumulated rewards\")\n",
    "\n",
    "    sns.lineplot(data=steps_df, x=\"Episodes\", y=\"Steps\", hue=\"map_size\", ax=ax[1])\n",
    "    ax[1].set(ylabel=\"Averaged steps number\")\n",
    "\n",
    "    for axi in ax:\n",
    "        axi.legend(title=\"map size\")\n",
    "    fig.tight_layout()\n",
    "    img_title = \"frozenlake_steps_and_rewards.png\"\n",
    "    fig.savefig(params.savefig_folder / img_title, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef41e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps_and_rewards(res_all, st_all) #cumulated rewards sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed380797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate_policy at regular intervals, plot avg reward over episodes\n",
    "evaluation_episodes = 100  # Number of episodes for evaluation\n",
    "evaluation_interval = 100  # Evaluate every 100 episodes\n",
    "average_rewards = []\n",
    "\n",
    "for i in range(0, params.total_episodes, evaluation_interval):\n",
    "    # Evaluate the policy\n",
    "    mean_reward, _ = evaluate_policy(\n",
    "        learner.qtable, env, n_eval_episodes=evaluation_episodes, deterministic=True\n",
    "    )\n",
    "    average_rewards.append((i, mean_reward))\n",
    "\n",
    "# Convert to a DataFrame for plotting\n",
    "evaluation_df = pd.DataFrame(average_rewards, columns=[\"Episodes\", \"Average Reward\"])\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=evaluation_df, x=\"Episodes\", y=\"Average Reward\")\n",
    "plt.title(\"Average Reward Over Episodes\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few random episodes\n",
    "for _ in range(5):\n",
    "    state = env.reset(seed=params.seed)[0]  # Reset the environment\n",
    "    done = False\n",
    "    print(\"Episode Start\")\n",
    "    while not done:\n",
    "        # Render the environment as a text-based grid\n",
    "        print(env.render(mode=\"ansi\"))\n",
    "        action = env.action_space.sample()  # Take a random action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    print(\"Episode End\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
